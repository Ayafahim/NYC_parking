{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Cell 1: Geocoding Parking Violations with Mapbox\n",
    "This cell samples 60,000 NYC parking violations, builds clean address strings, and geocodes them to latitude/longitude using the Mapbox API.\n",
    "To avoid geocoding the same address multiple times, it maintains a local geocode_cache.json that stores results.\n",
    "Coordinates are filtered to NYC bounds and exported to geocoded_fines_sample_50k.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Selected 59784 fines with 59784 unique addresses\n",
      "üìç Geocoding 0 new addresses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final geocode cache saved.\n",
      "‚úÖ Final dataset saved with 128,798 geocoded rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mapbox import Geocoder\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------\n",
    "MAPBOX_TOKEN = \"pk.eyJ1IjoiYXlhZmFoaW0iLCJhIjoiY21haDI0NzNtMDZnYjJrc2did2ozb2diMSJ9.ucfbzSq_1BEtyk7jqGJb1g\"\n",
    "CSV_PATH = \"data/nyc_parking_violations_sample.csv\"\n",
    "CACHE_PATH = \"geocode_cache.json\"\n",
    "BACKUP_PATH = \"geocode_cache_backup.json\"\n",
    "SAVE_EVERY = 1000\n",
    "SLEEP_TIME = 0.05\n",
    "SAMPLE_SIZE = 60000  # Target: 60k unique addresses\n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD & CLEAN DATA\n",
    "# ----------------------------------------\n",
    "cols = ['house_number', 'street_name', 'violation_county']\n",
    "df_raw = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "clean_df = df_raw.dropna(subset=cols).copy()\n",
    "\n",
    "# Clean up address parts\n",
    "clean_df['house_number'] = clean_df['house_number'].fillna('').astype(str).str.strip()\n",
    "clean_df['street_name'] = clean_df['street_name'].fillna('').astype(str).str.strip()\n",
    "clean_df['violation_county'] = clean_df['violation_county'].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Build full address\n",
    "clean_df['full_address'] = (\n",
    "    clean_df['house_number'] + \" \" +\n",
    "    clean_df['street_name'] + \", \" +\n",
    "    clean_df['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "# Remove addresses that are too short or invalid\n",
    "clean_df['full_address'] = clean_df['full_address'].str.replace('^\\\\s+', '', regex=True)\n",
    "clean_df = clean_df[clean_df['full_address'].str.len() > 10]\n",
    "\n",
    "\n",
    "# Shuffle to randomize address selection\n",
    "clean_df = clean_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Select 60k fines with unique addresses (1 per address)\n",
    "seen_addresses = set()\n",
    "selected_rows = []\n",
    "\n",
    "for _, row in clean_df.iterrows():\n",
    "    addr = row['full_address']\n",
    "    if addr not in seen_addresses:\n",
    "        selected_rows.append(row)\n",
    "        seen_addresses.add(addr)\n",
    "    if len(seen_addresses) >= SAMPLE_SIZE:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(selected_rows)\n",
    "print(f\"üì¶ Selected {len(df)} fines with {len(df['full_address'].unique())} unique addresses\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD OR CREATE CACHE\n",
    "# ----------------------------------------\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"r\") as f:\n",
    "        cache = json.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# ----------------------------------------\n",
    "# SETUP GEOCODER\n",
    "# ----------------------------------------\n",
    "geocoder = Geocoder(access_token=MAPBOX_TOKEN)\n",
    "\n",
    "# Filter uncached addresses\n",
    "unique_addresses = df['full_address'].unique()\n",
    "uncached = [addr for addr in unique_addresses if addr not in cache]\n",
    "print(f\"üìç Geocoding {len(uncached):,} new addresses...\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# GEOCODE LOOP\n",
    "# ----------------------------------------\n",
    "for i, addr in enumerate(tqdm(uncached)):\n",
    "    try:\n",
    "        response = geocoder.forward(addr, limit=1)\n",
    "        features = response.geojson().get('features', [])\n",
    "        if features:\n",
    "            coords = features[0]['geometry']['coordinates']\n",
    "            cache[addr] = [coords[1], coords[0]]  # lat, lon\n",
    "        else:\n",
    "            cache[addr] = [None, None]\n",
    "    except Exception:\n",
    "        cache[addr] = [None, None]\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (i + 1) % SAVE_EVERY == 0:\n",
    "        with open(CACHE_PATH, \"w\") as f:\n",
    "            json.dump(cache, f)\n",
    "        with open(BACKUP_PATH, \"w\") as f:\n",
    "            json.dump(cache, f)\n",
    "        print(f\"üíæ Checkpoint saved at {i+1} geocoded\")\n",
    "\n",
    "# Final cache save\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(cache, f)\n",
    "with open(BACKUP_PATH, \"w\") as f:\n",
    "    json.dump(cache, f)\n",
    "print(\"‚úÖ Final geocode cache saved.\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# APPLY GEOCOORDINATES TO SAMPLE\n",
    "# ----------------------------------------\n",
    "df[['lat', 'lon']] = df['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "df = df.dropna(subset=['lat', 'lon'])\n",
    "df = df[(df['lat'].between(40.49, 40.92)) & (df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "# ----------------------------------------\n",
    "# MERGE COORDS INTO FULL ORIGINAL DATA\n",
    "# ----------------------------------------\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].astype(str) + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "# Create full_address in full dataset\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].astype(str) + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "# Apply lat/lon from cache to ALL rows (not just sampled)\n",
    "df_raw[['lat', 'lon']] = df_raw['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "\n",
    "# Filter only valid NYC coordinates\n",
    "final_df = df_raw.dropna(subset=['lat', 'lon'])\n",
    "final_df = final_df[(final_df['lat'].between(40.49, 40.92)) & (final_df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "# Save\n",
    "final_df.to_csv(\"full_geocoded_parking_fines.csv\", index=False)\n",
    "print(f\"‚úÖ Final dataset saved with {len(final_df):,} geocoded rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final dataset saved with 49,878 geocoded rows (from cache only).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------\n",
    "CSV_PATH = \"data/nyc_parking_violations_sample.csv\"\n",
    "CACHE_PATH = \"geocode_cache.json\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD DATA & CACHE\n",
    "# ----------------------------------------\n",
    "df_raw = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    raise FileNotFoundError(\"Cache file not found. Run geocoder script first.\")\n",
    "\n",
    "with open(CACHE_PATH, \"r\") as f:\n",
    "    cache = json.load(f)\n",
    "\n",
    "# ----------------------------------------\n",
    "# BUILD FULL ADDRESS FIELD\n",
    "# ----------------------------------------\n",
    "df_raw['house_number'] = df_raw['house_number'].fillna('').astype(str).str.strip()\n",
    "df_raw['street_name'] = df_raw['street_name'].fillna('').astype(str).str.strip()\n",
    "df_raw['violation_county'] = df_raw['violation_county'].fillna('').astype(str).str.strip()\n",
    "\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'] + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ").str.replace('^\\\\s+', '', regex=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# APPLY CACHED GEOCOORDINATES\n",
    "# ----------------------------------------\n",
    "df_raw[['lat', 'lon']] = df_raw['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "\n",
    "# ----------------------------------------\n",
    "# FILTER VALID NYC LOCATIONS\n",
    "# ----------------------------------------\n",
    "final_df = df_raw.dropna(subset=['lat', 'lon'])\n",
    "final_df = final_df[\n",
    "    (final_df['lat'].between(40.49, 40.92)) &\n",
    "    (final_df['lon'].between(-74.26, -73.68))\n",
    "]\n",
    "\n",
    "# ----------------------------------------\n",
    "# SAVE FINAL DATASET\n",
    "# ----------------------------------------\n",
    "final_df.to_csv(\"full_geocoded_parking_fines.csv\", index=False)\n",
    "print(f\"‚úÖ Final dataset saved with {len(final_df):,} geocoded rows (from cache only).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ 129,626 addresses in this dataset match the cache.\n"
     ]
    }
   ],
   "source": [
    "# Load the cache\n",
    "with open(\"geocode_cache.json\") as f:\n",
    "    cache = json.load(f)\n",
    "\n",
    "# Build full_address in your 80K dataset\n",
    "df_raw = pd.read_csv(\"data/nyc_parking_violations_sample.csv\", low_memory=False)\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].fillna('').astype(str).str.strip() + \" \" +\n",
    "    df_raw['street_name'].fillna('').astype(str).str.strip() + \", \" +\n",
    "    df_raw['violation_county'].fillna('').astype(str).str.strip() + \", NYC\"\n",
    ")\n",
    "\n",
    "# How many full_address values in df_raw match the cache?\n",
    "matching = df_raw['full_address'].isin(cache.keys()).sum()\n",
    "print(f\"üéØ {matching:,} addresses in this dataset match the cache.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Cell 2: Spatial Join ‚Äì Mapping Fines to Neighborhoods\n",
    "This cell reads the geocoded fines and converts them into a GeoDataFrame.\n",
    "It then loads the Neighborhood Tabulation Areas (NTA) from a GeoJSON file, reprojects it to the same coordinate system, and performs a spatial join to attach each fine to a neighborhood (if it intersects spatially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  house_number     street_name   boroname                       ntaname\n",
      "0           51       E 44TH ST  Manhattan          Midtown-Times Square\n",
      "1        39-41         60TH ST     Queens                      Woodside\n",
      "2       126 05          36 AVE     Queens                 College Point\n",
      "3           41      SEAVER WAY     Queens  Flushing Meadows-Corona Park\n",
      "4        46-50  BURLING STREET     Queens                 East Flushing\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Load geocoded fine data\n",
    "df = pd.read_csv(\"full_geocoded_parking_fines.csv\", low_memory=False)\n",
    "\n",
    "# Filter to NYC bounds (just in case)\n",
    "# df = df[(df['lat'].between(40.49, 40.92)) & (df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df['lon'], df['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Load the clean GeoJSON instead of the shapefile\n",
    "nta = gpd.read_file(\"data/nynta2020.geojson\")  # replace path if needed\n",
    "nta = nta.to_crs(\"EPSG:4326\")  # ensure same CRS\n",
    "\n",
    "# Spatial join: fines ‚Üí neighborhoods\n",
    "joined = gpd.sjoin(gdf, nta, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "\n",
    "# Preview matched fines with neighborhood info\n",
    "print(joined[['house_number', 'street_name', 'boroname', 'ntaname']].dropna().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cell 3: Count Fines by Neighborhood\n",
    "Here, we group the spatially joined data by ntaname (neighborhood name) and count how many fines occurred in each neighborhood.\n",
    "This gives us raw counts of violations by area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 ntaname  num_fines\n",
      "65                          East Village       2226\n",
      "30                              Canarsie       2165\n",
      "35                  Chelsea-Hudson Yards       2102\n",
      "211        Upper East Side-Carnegie Hill       2048\n",
      "137                 Midtown-Times Square       1999\n",
      "51   Downtown Brooklyn-DUMBO-Boerum Hill       1997\n",
      "136  Midtown South-Flatiron-Union Square       1962\n",
      "214            Upper West Side (Central)       1919\n",
      "112                              Jamaica       1756\n",
      "19                          Borough Park       1746\n"
     ]
    }
   ],
   "source": [
    "# Count number of fines per neighborhood\n",
    "nta_counts = joined.groupby('ntaname').size().reset_index(name='num_fines')\n",
    "\n",
    "# Preview top 10\n",
    "print(nta_counts.sort_values(by='num_fines', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë• Cell 4: Normalize by Population\n",
    "We load NYC population data by NTA and merge it with the fine data by matching on neighborhood names.\n",
    "Then we compute fines per 1,000 residents, which normalizes the violation counts relative to population size ‚Äî giving a fairer comparison across neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ntaname   ntaabbrev   boroname  Population  num_fines  \\\n",
      "38        East Village      EstVlg  Manhattan     41746.0       2226   \n",
      "39        East Village      EstVlg  Manhattan     44136.0       2226   \n",
      "28   Crotona Park East  CrtnaPkEst      Bronx     18072.0        692   \n",
      "150       Williamsburg    Wllmsbrg   Brooklyn     32269.0       1199   \n",
      "151       Williamsburg    Wllmsbrg   Brooklyn     32926.0       1199   \n",
      "\n",
      "     fines_per_1000  \n",
      "38        53.322474  \n",
      "39        50.435019  \n",
      "28        38.291279  \n",
      "150       37.156404  \n",
      "151       36.414991  \n"
     ]
    }
   ],
   "source": [
    "pop_df = pd.read_csv(\"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\")\n",
    "\n",
    "# Strip whitespace and normalize\n",
    "joined['ntaname'] = joined['ntaname'].str.strip()\n",
    "pop_df['NTA Name'] = pop_df['NTA Name'].str.strip()\n",
    "\n",
    "# Merge using ntaname\n",
    "merged = joined.merge(pop_df, left_on='ntaname', right_on='NTA Name', how='left')\n",
    "\n",
    "# Group and compute stats\n",
    "nta_stats = (\n",
    "    merged.groupby(['ntaname', 'ntaabbrev', 'boroname', 'Population'])\n",
    "    .size()\n",
    "    .reset_index(name='num_fines')\n",
    ")\n",
    "\n",
    "nta_stats['fines_per_1000'] = (nta_stats['num_fines'] / nta_stats['Population']) * 1000\n",
    "\n",
    "# Preview\n",
    "print(nta_stats.sort_values(by='fines_per_1000', ascending=False).head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cell 5: Clean and Aggregate Neighborhood Stats\n",
    "Some neighborhoods may appear multiple times (due to data duplication or merged borders).\n",
    "This step groups by neighborhood name again and computes the mean fines per 1,000 residents for each neighborhood to get a clean final dataset for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ntaname  num_fines  Population  fines_per_1000\n",
      "19                   East Village       4452     42941.0       51.878747\n",
      "75                   Williamsburg       2398     32597.5       36.785698\n",
      "14              Crotona Park East       1384     19174.5       36.209308\n",
      "37                        Jamaica       3512     52800.0       33.268368\n",
      "72  Upper East Side-Carnegie Hill       4096     62435.5       32.814556\n"
     ]
    }
   ],
   "source": [
    "nta_cleaned = (\n",
    "    nta_stats.groupby('ntaname')\n",
    "    .agg({\n",
    "        'num_fines': 'sum',\n",
    "        'Population': 'mean',\n",
    "        'fines_per_1000': 'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .sort_values(by='fines_per_1000', ascending=False)\n",
    ")\n",
    "print(nta_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned neighborhood-level stats\n",
    "nta_cleaned.to_csv(\"data/nta_fine_stats_cleaned.csv\", index=False)\n",
    "\n",
    "# Also save raw joined fine points (optional)\n",
    "# joined.to_file(\"data/fines_joined_with_neighborhoods.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# Save GeoJSON with stats merged in (for choropleth)\n",
    "geo_merged = nta.merge(nta_cleaned, on='ntaname', how='left')\n",
    "geo_merged.to_file(\"data/nta_with_fine_stats.geojson\", driver=\"GeoJSON\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

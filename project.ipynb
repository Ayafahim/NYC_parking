{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Cell 1: Geocoding Parking Violations with Mapbox\n",
    "This cell samples 60,000 NYC parking violations, builds clean address strings, and geocodes them to latitude/longitude using the Mapbox API.\n",
    "To avoid geocoding the same address multiple times, it maintains a local geocode_cache.json that stores results.\n",
    "Coordinates are filtered to NYC bounds and exported to geocoded_fines_sample_50k.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mapbox import Geocoder\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------\n",
    "MAPBOX_TOKEN = \"pk.eyJ1IjoiYXlhZmFoaW0iLCJhIjoiY21haDI0NzNtMDZnYjJrc2did2ozb2diMSJ9.ucfbzSq_1BEtyk7jqGJb1g\"\n",
    "CSV_PATH = \"data/nyc_parking_violations_sample.csv\"\n",
    "CACHE_PATH = \"geocode_cache.json\"\n",
    "BACKUP_PATH = \"geocode_cache_backup.json\"\n",
    "SAVE_EVERY = 1000\n",
    "SLEEP_TIME = 0.05\n",
    "SAMPLE_SIZE = 60000  # Target: 60k unique addresses\n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD & CLEAN DATA\n",
    "# ----------------------------------------\n",
    "cols = ['house_number', 'street_name', 'violation_county']\n",
    "df_raw = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "clean_df = df_raw.dropna(subset=cols).copy()\n",
    "\n",
    "# Clean up address parts\n",
    "clean_df['house_number'] = clean_df['house_number'].fillna('').astype(str).str.strip()\n",
    "clean_df['street_name'] = clean_df['street_name'].fillna('').astype(str).str.strip()\n",
    "clean_df['violation_county'] = clean_df['violation_county'].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Build full address\n",
    "clean_df['full_address'] = (\n",
    "    clean_df['house_number'] + \" \" +\n",
    "    clean_df['street_name'] + \", \" +\n",
    "    clean_df['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "# Remove addresses that are too short or invalid\n",
    "clean_df['full_address'] = clean_df['full_address'].str.replace('^\\\\s+', '', regex=True)\n",
    "clean_df = clean_df[clean_df['full_address'].str.len() > 10]\n",
    "\n",
    "\n",
    "# Shuffle to randomize address selection\n",
    "clean_df = clean_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Select 60k fines with unique addresses (1 per address)\n",
    "seen_addresses = set()\n",
    "selected_rows = []\n",
    "\n",
    "for _, row in clean_df.iterrows():\n",
    "    addr = row['full_address']\n",
    "    if addr not in seen_addresses:\n",
    "        selected_rows.append(row)\n",
    "        seen_addresses.add(addr)\n",
    "    if len(seen_addresses) >= SAMPLE_SIZE:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(selected_rows)\n",
    "print(f\"üì¶ Selected {len(df)} fines with {len(df['full_address'].unique())} unique addresses\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD OR CREATE CACHE\n",
    "# ----------------------------------------\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"r\") as f:\n",
    "        cache = json.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# ----------------------------------------\n",
    "# SETUP GEOCODER\n",
    "# ----------------------------------------\n",
    "geocoder = Geocoder(access_token=MAPBOX_TOKEN)\n",
    "\n",
    "# Filter uncached addresses\n",
    "unique_addresses = df['full_address'].unique()\n",
    "uncached = [addr for addr in unique_addresses if addr not in cache]\n",
    "print(f\"üìç Geocoding {len(uncached):,} new addresses...\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# GEOCODE LOOP\n",
    "# ----------------------------------------\n",
    "for i, addr in enumerate(tqdm(uncached)):\n",
    "    try:\n",
    "        response = geocoder.forward(addr, limit=1)\n",
    "        features = response.geojson().get('features', [])\n",
    "        if features:\n",
    "            coords = features[0]['geometry']['coordinates']\n",
    "            cache[addr] = [coords[1], coords[0]]  # lat, lon\n",
    "        else:\n",
    "            cache[addr] = [None, None]\n",
    "    except Exception:\n",
    "        cache[addr] = [None, None]\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (i + 1) % SAVE_EVERY == 0:\n",
    "        with open(CACHE_PATH, \"w\") as f:\n",
    "            json.dump(cache, f)\n",
    "        with open(BACKUP_PATH, \"w\") as f:\n",
    "            json.dump(cache, f)\n",
    "        print(f\"üíæ Checkpoint saved at {i+1} geocoded\")\n",
    "\n",
    "# Final cache save\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(cache, f)\n",
    "with open(BACKUP_PATH, \"w\") as f:\n",
    "    json.dump(cache, f)\n",
    "print(\"‚úÖ Final geocode cache saved.\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# APPLY GEOCOORDINATES TO SAMPLE\n",
    "# ----------------------------------------\n",
    "df[['lat', 'lon']] = df['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "df = df.dropna(subset=['lat', 'lon'])\n",
    "df = df[(df['lat'].between(40.49, 40.92)) & (df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "# ----------------------------------------\n",
    "# MERGE COORDS INTO FULL ORIGINAL DATA\n",
    "# ----------------------------------------\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].astype(str) + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "# Create full_address in full dataset\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].astype(str) + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "# Apply lat/lon from cache to ALL rows (not just sampled)\n",
    "df_raw[['lat', 'lon']] = df_raw['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "\n",
    "# Filter only valid NYC coordinates\n",
    "final_df = df_raw.dropna(subset=['lat', 'lon'])\n",
    "final_df = final_df[(final_df['lat'].between(40.49, 40.92)) & (final_df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "# Save\n",
    "final_df.to_csv(\"full_geocoded_parking_fines.csv\", index=False)\n",
    "print(f\"‚úÖ Final dataset saved with {len(final_df):,} geocoded rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final dataset saved with 40,412 geocoded rows (from cache only).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------\n",
    "CSV_PATH = \"data/nyc_parking_violations_sample.csv\"\n",
    "CACHE_PATH = \"geocode_cache.json\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD DATA & CACHE\n",
    "# ----------------------------------------\n",
    "df_raw = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    raise FileNotFoundError(\"Cache file not found. Run geocoder script first.\")\n",
    "\n",
    "with open(CACHE_PATH, \"r\") as f:\n",
    "    cache = json.load(f)\n",
    "\n",
    "# ----------------------------------------\n",
    "# BUILD FULL ADDRESS FIELD\n",
    "# ----------------------------------------\n",
    "df_raw['house_number'] = df_raw['house_number'].fillna('').astype(str).str.strip()\n",
    "df_raw['street_name'] = df_raw['street_name'].fillna('').astype(str).str.strip()\n",
    "df_raw['violation_county'] = df_raw['violation_county'].fillna('').astype(str).str.strip()\n",
    "\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'] + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ").str.replace('^\\\\s+', '', regex=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# APPLY CACHED GEOCOORDINATES\n",
    "# ----------------------------------------\n",
    "df_raw[['lat', 'lon']] = df_raw['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "\n",
    "# ----------------------------------------\n",
    "# FILTER VALID NYC LOCATIONS\n",
    "# ----------------------------------------\n",
    "final_df = df_raw.dropna(subset=['lat', 'lon'])\n",
    "final_df = final_df[\n",
    "    (final_df['lat'].between(40.49, 40.92)) &\n",
    "    (final_df['lon'].between(-74.26, -73.68))\n",
    "]\n",
    "\n",
    "# ----------------------------------------\n",
    "# SAVE FINAL DATASET\n",
    "# ----------------------------------------\n",
    "final_df.to_csv(\"full_geocoded_parking_fines.csv\", index=False)\n",
    "print(f\"‚úÖ Final dataset saved with {len(final_df):,} geocoded rows (from cache only).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ 21,627 addresses in this dataset match the cache.\n"
     ]
    }
   ],
   "source": [
    "# Load the cache\n",
    "with open(\"geocode_cache.json\") as f:\n",
    "    cache = json.load(f)\n",
    "\n",
    "# Build full_address in your 80K dataset\n",
    "df_raw = pd.read_csv(\"data/nyc_parking_violations_sample.csv\", low_memory=False)\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].fillna('').astype(str).str.strip() + \" \" +\n",
    "    df_raw['street_name'].fillna('').astype(str).str.strip() + \", \" +\n",
    "    df_raw['violation_county'].fillna('').astype(str).str.strip() + \", NYC\"\n",
    ")\n",
    "\n",
    "# How many full_address values in df_raw match the cache?\n",
    "matching = df_raw['full_address'].isin(cache.keys()).sum()\n",
    "print(f\"üéØ {matching:,} addresses in this dataset match the cache.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nan I/O TAYLOR AVE, BX, NYC', '51 E 44TH ST, NY, NYC', '39-41 60TH ST, Q, NYC', 'nan C/O 126 ST, Q, NYC', '126 05 36 AVE, Q, NYC', 'nan STADIUM P N, Q, NYC', '41 SEAVER WAY, Q, NYC', 'nan PRINCE STREET, Q, NYC', '46-50 BURLING STREET, Q, NYC', '46-08 161 ST, Q, NYC']\n"
     ]
    }
   ],
   "source": [
    "print(list(cache.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Cell 2: Spatial Join ‚Äì Mapping Fines to Neighborhoods\n",
    "This cell reads the geocoded fines and converts them into a GeoDataFrame.\n",
    "It then loads the Neighborhood Tabulation Areas (NTA) from a GeoJSON file, reprojects it to the same coordinate system, and performs a spatial join to attach each fine to a neighborhood (if it intersects spatially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  house_number    street_name   boroname                              ntaname\n",
      "0          290       Broadway   Brooklyn                         Williamsburg\n",
      "1            E  Lefferts Blvd     Queens                     South Ozone Park\n",
      "2          489       LENOX RD   Brooklyn                East Flatbush-Erasmus\n",
      "3           30      E 18th St  Manhattan  Midtown South-Flatiron-Union Square\n",
      "4          320   Atlantic Ave   Brooklyn  Downtown Brooklyn-DUMBO-Boerum Hill\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Load geocoded fine data\n",
    "df = pd.read_csv(\"geocoded_fines_sample_50k.csv\")\n",
    "\n",
    "# Filter to NYC bounds (just in case)\n",
    "# df = df[(df['lat'].between(40.49, 40.92)) & (df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df['lon'], df['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Load the clean GeoJSON instead of the shapefile\n",
    "nta = gpd.read_file(\"data/nynta2020.geojson\")  # replace path if needed\n",
    "nta = nta.to_crs(\"EPSG:4326\")  # ensure same CRS\n",
    "\n",
    "# Spatial join: fines ‚Üí neighborhoods\n",
    "joined = gpd.sjoin(gdf, nta, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "\n",
    "# Preview matched fines with neighborhood info\n",
    "print(joined[['house_number', 'street_name', 'boroname', 'ntaname']].dropna().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cell 3: Count Fines by Neighborhood\n",
    "Here, we group the spatially joined data by ntaname (neighborhood name) and count how many fines occurred in each neighborhood.\n",
    "This gives us raw counts of violations by area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 ntaname  num_fines\n",
      "131                 Midtown-Times Square       1098\n",
      "32                  Chelsea-Hudson Yards        673\n",
      "88                     Greenwich Village        669\n",
      "204        Upper East Side-Carnegie Hill        651\n",
      "207            Upper West Side (Central)        649\n",
      "61                          East Village        626\n",
      "130  Midtown South-Flatiron-Union Square        608\n",
      "216                         West Village        603\n",
      "93                        Hell's Kitchen        493\n",
      "142                 Murray Hill-Kips Bay        488\n"
     ]
    }
   ],
   "source": [
    "# Count number of fines per neighborhood\n",
    "nta_counts = joined.groupby('ntaname').size().reset_index(name='num_fines')\n",
    "\n",
    "# Preview top 10\n",
    "print(nta_counts.sort_values(by='num_fines', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë• Cell 4: Normalize by Population\n",
    "We load NYC population data by NTA and merge it with the fine data by matching on neighborhood names.\n",
    "Then we compute fines per 1,000 residents, which normalizes the violation counts relative to population size ‚Äî giving a fairer comparison across neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           ntaname   ntaabbrev   boroname  Population  \\\n",
      "38                    East Village      EstVlg  Manhattan     41746.0   \n",
      "39                    East Village      EstVlg  Manhattan     44136.0   \n",
      "62                        Gramercy      Grmrcy  Manhattan     26184.0   \n",
      "63                        Gramercy      Grmrcy  Manhattan     27988.0   \n",
      "144  Upper East Side-Carnegie Hill  UES_CrngHl  Manhattan     61207.0   \n",
      "\n",
      "     num_fines  fines_per_1000  \n",
      "38         626       14.995449  \n",
      "39         626       14.183433  \n",
      "62         299       11.419187  \n",
      "63         299       10.683150  \n",
      "144        651       10.636038  \n"
     ]
    }
   ],
   "source": [
    "pop_df = pd.read_csv(\"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\")\n",
    "\n",
    "# Strip whitespace and normalize\n",
    "joined['ntaname'] = joined['ntaname'].str.strip()\n",
    "pop_df['NTA Name'] = pop_df['NTA Name'].str.strip()\n",
    "\n",
    "# Merge using ntaname\n",
    "merged = joined.merge(pop_df, left_on='ntaname', right_on='NTA Name', how='left')\n",
    "\n",
    "# Group and compute stats\n",
    "nta_stats = (\n",
    "    merged.groupby(['ntaname', 'ntaabbrev', 'boroname', 'Population'])\n",
    "    .size()\n",
    "    .reset_index(name='num_fines')\n",
    ")\n",
    "\n",
    "nta_stats['fines_per_1000'] = (nta_stats['num_fines'] / nta_stats['Population']) * 1000\n",
    "\n",
    "# Preview\n",
    "print(nta_stats.sort_values(by='fines_per_1000', ascending=False).head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cell 5: Clean and Aggregate Neighborhood Stats\n",
    "Some neighborhoods may appear multiple times (due to data duplication or merged borders).\n",
    "This step groups by neighborhood name again and computes the mean fines per 1,000 residents for each neighborhood to get a clean final dataset for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ntaname  num_fines  Population  fines_per_1000\n",
      "19                   East Village       1252     42941.0       14.589441\n",
      "31                       Gramercy        598     27086.0       11.051169\n",
      "72  Upper East Side-Carnegie Hill       1302     62435.5       10.430799\n",
      "51           Murray Hill-Kips Bay        976     49580.5        9.847984\n",
      "73                   West Village       1206     67681.5        8.910627\n"
     ]
    }
   ],
   "source": [
    "nta_cleaned = (\n",
    "    nta_stats.groupby('ntaname')\n",
    "    .agg({\n",
    "        'num_fines': 'sum',\n",
    "        'Population': 'mean',\n",
    "        'fines_per_1000': 'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .sort_values(by='fines_per_1000', ascending=False)\n",
    ")\n",
    "print(nta_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned neighborhood-level stats\n",
    "nta_cleaned.to_csv(\"data/nta_fine_stats_cleaned.csv\", index=False)\n",
    "\n",
    "# Also save raw joined fine points (optional)\n",
    "joined.to_file(\"data/fines_joined_with_neighborhoods.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# Save GeoJSON with stats merged in (for choropleth)\n",
    "geo_merged = nta.merge(nta_cleaned, on='ntaname', how='left')\n",
    "geo_merged.to_file(\"data/nta_with_fine_stats.geojson\", driver=\"GeoJSON\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

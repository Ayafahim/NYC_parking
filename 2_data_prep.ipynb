{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding Parking Violations with Mapbox\n",
    "This cell samples 60,000 NYC parking violations, builds clean address strings, and geocodes them to latitude/longitude using the Mapbox API.\n",
    "To avoid geocoding the same address multiple times, it maintains a local geocode_cache.json that stores results.\n",
    "Coordinates are filtered to NYC bounds and exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Selected 59784 fines with 59784 unique addresses\n",
      "üìç Geocoding 0 new addresses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final geocode cache saved.\n",
      "‚úÖ Final dataset saved with 128,798 geocoded rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mapbox import Geocoder\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------\n",
    "MAPBOX_TOKEN = \"pk.eyJ1IjoiYXlhZmFoaW0iLCJhIjoiY21haDI0NzNtMDZnYjJrc2did2ozb2diMSJ9.ucfbzSq_1BEtyk7jqGJb1g\"\n",
    "CSV_PATH = \"data/nyc_parking_violations_sample.csv\"\n",
    "CACHE_PATH = \"geocode_cache.json\"\n",
    "BACKUP_PATH = \"geocode_cache_backup.json\"\n",
    "SAVE_EVERY = 1000\n",
    "SLEEP_TIME = 0.05\n",
    "SAMPLE_SIZE = 60000 \n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD & CLEAN DATA\n",
    "# ----------------------------------------\n",
    "cols = ['house_number', 'street_name', 'violation_county']\n",
    "df_raw = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "clean_df = df_raw.dropna(subset=cols).copy()\n",
    "\n",
    "# Clean up address parts\n",
    "clean_df['house_number'] = clean_df['house_number'].fillna('').astype(str).str.strip()\n",
    "clean_df['street_name'] = clean_df['street_name'].fillna('').astype(str).str.strip()\n",
    "clean_df['violation_county'] = clean_df['violation_county'].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Build full address\n",
    "clean_df['full_address'] = (\n",
    "    clean_df['house_number'] + \" \" +\n",
    "    clean_df['street_name'] + \", \" +\n",
    "    clean_df['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "clean_df['full_address'] = clean_df['full_address'].str.replace('^\\\\s+', '', regex=True)\n",
    "clean_df = clean_df[clean_df['full_address'].str.len() > 10]\n",
    "\n",
    "\n",
    "# Shuffle to randomize address selection\n",
    "clean_df = clean_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Select 60k fines with unique addresses (1 per address)\n",
    "seen_addresses = set()\n",
    "selected_rows = []\n",
    "\n",
    "for _, row in clean_df.iterrows():\n",
    "    addr = row['full_address']\n",
    "    if addr not in seen_addresses:\n",
    "        selected_rows.append(row)\n",
    "        seen_addresses.add(addr)\n",
    "    if len(seen_addresses) >= SAMPLE_SIZE:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(selected_rows)\n",
    "print(f\"üì¶ Selected {len(df)} fines with {len(df['full_address'].unique())} unique addresses\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# LOAD OR CREATE CACHE\n",
    "# ----------------------------------------\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"r\") as f:\n",
    "        cache = json.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# ----------------------------------------\n",
    "# SETUP GEOCODER\n",
    "# ----------------------------------------\n",
    "geocoder = Geocoder(access_token=MAPBOX_TOKEN)\n",
    "\n",
    "# Filter uncached addresses\n",
    "unique_addresses = df['full_address'].unique()\n",
    "uncached = [addr for addr in unique_addresses if addr not in cache]\n",
    "print(f\"üìç Geocoding {len(uncached):,} new addresses...\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# GEOCODE LOOP\n",
    "# ----------------------------------------\n",
    "for i, addr in enumerate(tqdm(uncached)):\n",
    "    try:\n",
    "        response = geocoder.forward(addr, limit=1)\n",
    "        features = response.geojson().get('features', [])\n",
    "        if features:\n",
    "            coords = features[0]['geometry']['coordinates']\n",
    "            cache[addr] = [coords[1], coords[0]]  # lat, lon\n",
    "        else:\n",
    "            cache[addr] = [None, None]\n",
    "    except Exception:\n",
    "        cache[addr] = [None, None]\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (i + 1) % SAVE_EVERY == 0:\n",
    "        with open(CACHE_PATH, \"w\") as f:\n",
    "            json.dump(cache, f)\n",
    "        with open(BACKUP_PATH, \"w\") as f:\n",
    "            json.dump(cache, f)\n",
    "        print(f\"üíæ Checkpoint saved at {i+1} geocoded\")\n",
    "\n",
    "# Final cache save\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(cache, f)\n",
    "with open(BACKUP_PATH, \"w\") as f:\n",
    "    json.dump(cache, f)\n",
    "print(\"‚úÖ Final geocode cache saved.\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# APPLY GEOCOORDINATES TO SAMPLE\n",
    "# ----------------------------------------\n",
    "df[['lat', 'lon']] = df['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "df = df.dropna(subset=['lat', 'lon'])\n",
    "df = df[(df['lat'].between(40.49, 40.92)) & (df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "# ----------------------------------------\n",
    "# MERGE COORDS INTO FULL ORIGINAL DATA\n",
    "# ----------------------------------------\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].astype(str) + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "df_raw['full_address'] = (\n",
    "    df_raw['house_number'].astype(str) + \" \" +\n",
    "    df_raw['street_name'] + \", \" +\n",
    "    df_raw['violation_county'] + \", NYC\"\n",
    ")\n",
    "\n",
    "df_raw[['lat', 'lon']] = df_raw['full_address'].apply(lambda x: pd.Series(cache.get(x, [None, None])))\n",
    "\n",
    "# Filter only valid NYC coordinates\n",
    "final_df = df_raw.dropna(subset=['lat', 'lon'])\n",
    "final_df = final_df[(final_df['lat'].between(40.49, 40.92)) & (final_df['lon'].between(-74.26, -73.68))]\n",
    "\n",
    "final_df.to_csv(\"full_geocoded_parking_fines.csv\", index=False)\n",
    "print(f\"‚úÖ Final dataset saved with {len(final_df):,} geocoded rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare 2020 NTA Population Data\n",
    "\n",
    "We take U.S. Census population data by tract and use a crosswalk file to map each tract to a 2020 NTA (Neighborhood Tabulation Area). Then we sum the populations within each NTA and save the total population per NTA for later use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    NTACode                    NTAName  Population\n",
      "31   BK1001                  Bay Ridge      331906\n",
      "9    BK0301  Bedford-Stuyvesant (West)      310028\n",
      "10   BK0302  Bedford-Stuyvesant (East)      302127\n",
      "39   BK1202               Borough Park      255691\n",
      "170  QN0301            Jackson Heights      232256\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Census tract population\n",
    "pop_df = pd.read_csv(\"data/nyc_census_tract_pop_2020.csv\")\n",
    "pop_df.rename(columns={'P1_001N': 'Population', 'tract': 'CT2020'}, inplace=True)\n",
    "pop_df['Population'] = pd.to_numeric(pop_df['Population'], errors='coerce')\n",
    "pop_df['CT2020'] = pop_df['CT2020'].astype(str).str.zfill(6)  # Ensure leading zeros\n",
    "\n",
    "# Load tract-to-NTA crosswalk\n",
    "crosswalk = pd.read_csv(\"data/2020_Census_Tracts_to_2020_NTAs_and_CDTAs_Equivalency_20250511.csv\")\n",
    "crosswalk['CT2020'] = crosswalk['CT2020'].astype(str).str.zfill(6)\n",
    "\n",
    "# Merge Census + crosswalk\n",
    "merged = pop_df.merge(crosswalk, on='CT2020', how='left')\n",
    "\n",
    "# Group by NTA\n",
    "nta_pop = (\n",
    "    merged.groupby(['NTACode', 'NTAName'])\n",
    "    .agg({'Population': 'sum'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Save for later use\n",
    "nta_pop.to_csv(\"data/nta_population_2020.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(nta_pop.sort_values(by=\"Population\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Parking Fines to Neighborhoods and Calculate Fines Per 1,000 Residents\n",
    "\n",
    "We spatially join geocoded parking fines to the 2020 NTA boundaries, then combine that with the NTA population data to calculate the number of parking fines per 1,000 residents in each neighborhood. The results are saved for use in maps and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  house_number     street_name   boroname                       ntaname\n",
      "0           51       E 44TH ST  Manhattan          Midtown-Times Square\n",
      "1        39-41         60TH ST     Queens                      Woodside\n",
      "2       126 05          36 AVE     Queens                 College Point\n",
      "3           41      SEAVER WAY     Queens  Flushing Meadows-Corona Park\n",
      "4        46-50  BURLING STREET     Queens                 East Flushing\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Load geocoded fine data\n",
    "df = pd.read_csv(\"full_geocoded_parking_fines.csv\", low_memory=False)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df['lon'], df['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Load updated 2020 NTA GeoJSON\n",
    "nta = gpd.read_file(\"data/nynta2020.geojson\").to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Spatial join: assign each fine to a neighborhood\n",
    "joined = gpd.sjoin(gdf, nta, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# ‚úÖ Preview matched data\n",
    "print(joined[['house_number', 'street_name', 'boroname', 'ntaname']].dropna().head())\n",
    "\n",
    "# Load updated population data\n",
    "pop_df = pd.read_csv(\"data/nta_population_2020.csv\")\n",
    "pop_df.rename(columns={'NTAName': 'ntaname', 'NTACode': 'nta2020'}, inplace=True)\n",
    "pop_df['ntaname'] = pop_df['ntaname'].str.strip()\n",
    "joined['ntaname'] = joined['ntaname'].str.strip()\n",
    "\n",
    "# Merge fines with population\n",
    "merged = joined.merge(pop_df, on='ntaname', how='left')\n",
    "\n",
    "# Group by NTA and calculate fines per 1,000 residents\n",
    "nta_stats = (\n",
    "    merged.groupby(['ntaname', 'ntaabbrev', 'boroname', 'Population'])\n",
    "    .size()\n",
    "    .reset_index(name='num_fines')\n",
    ")\n",
    "nta_stats['fines_per_1000'] = (nta_stats['num_fines'] / nta_stats['Population']) * 1000\n",
    "\n",
    "# Cleaned version for visualization\n",
    "nta_cleaned = (\n",
    "    nta_stats.groupby('ntaname')\n",
    "    .agg({\n",
    "        'num_fines': 'sum',\n",
    "        'Population': 'mean',\n",
    "        'fines_per_1000': 'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .sort_values(by='fines_per_1000', ascending=False)\n",
    ")\n",
    "\n",
    "# Save cleaned stats\n",
    "nta_cleaned.to_csv(\"data/nta_fine_stats_cleaned.csv\", index=False)\n",
    "\n",
    "# Save GeoJSON with stats for choropleth mapping\n",
    "geo_merged = nta.merge(nta_cleaned, on='ntaname', how='left')\n",
    "geo_merged.to_file(\"data/nta_with_fine_stats.geojson\", driver=\"GeoJSON\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
